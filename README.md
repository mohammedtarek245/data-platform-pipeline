#  Local Event-Driven Data Platform ‚Äî Kafka, Spark, Airflow

A modular **offline-first ETL platform** built for simulating real-time event ingestion from diverse sources like CRM, ERP, websites, and mobile apps. The pipeline uses **Kafka** for event transport, **PySpark** for transformation, and **Airflow** for orchestration ‚Äî all running locally via Docker.

---

##  Who Is This For?

-  **Data Engineers** learning Kafka, PySpark, and Airflow in a fully offline, local setup.
-  **Real Estate Tech Teams** needing a prototype ETL backbone for operational data.
-  **Students or Bootcampers** building hands-on experience in streaming pipelines.

---

## High-Level Architecture

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Kafka        ‚îÇ --> ‚îÇ Kafka Topics ‚îÇ --> ‚îÇ Spark Structured   ‚îÇ --> ‚îÇ JSON Output & Archiving      ‚îÇ --> ‚îÇ Airflow DAG        ‚îÇ --> ‚îÇ Reporting Tools    ‚îÇ
‚îÇ Producers    ‚îÇ     ‚îÇ (CRM, ERP,   ‚îÇ     ‚îÇ Streaming Consumer ‚îÇ     ‚îÇ  ‚Ä¢ /output/ JSON             ‚îÇ     ‚îÇ (Daily @ 2PMcairo time)     ‚îÇ     ‚îÇ (Optional ‚Äì PowerBI‚îÇ
‚îÇ (Mock Events)‚îÇ     ‚îÇ Website, App)‚îÇ     ‚îÇ   ‚Ä¢ Parse / Clean  ‚îÇ     ‚îÇ  ‚Ä¢ /archive/ by timestamp    ‚îÇ     ‚îÇ 1. produce         ‚îÇ     ‚îÇ  or Excel, etc.)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚ñ≤                                                                                                                            ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Controlled via Airflow DAG Scheduler ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


![Pipeline Architecture](./docs/architecture.png)

---

Screenshots:n
 
 1. Terminal ‚Äì Running topics.py

Kafka topics are created and confirmed via terminal logs:

$ python dags/topics.py

![Terminal Topics](./docs/screenshots/topics_created.png)

 2. Terminal ‚Äì Running producers.py

Mock events are successfully pushed into Kafka topics:

$ python dags/producers/producers.py
![Terminal Producers](./docs/screenshots/producers_running.png)



3. Airflow DAG Triggered Manually

This shows the DAG triggered from the UI and successfully executing tasks (producing events, transforming with Spark, and storing output):
![Airflow DAG Triggered](./docs/screenshots/airflow_dag_success.png)


##  Tech Stack

- **Apache Kafka** ‚Äì Topic-based event queue
- **PySpark** ‚Äì Structured streaming + transformation
- **Apache Airflow** ‚Äì Scheduling and workflow orchestration
- **PostgreSQL / JSON** ‚Äì Output targets for cleaned data
- **Docker Compose** ‚Äì Local orchestration for all services

---

##  Project Structure

```bash
project-root/
‚îú‚îÄ‚îÄ docker-compose.yml              # Defines Kafka, Airflow, Spark, Postgres containers
‚îú‚îÄ‚îÄ .env                            # Env variables (e.g., email alerts)
‚îú‚îÄ‚îÄ output/                         # Transformed JSON output
‚îú‚îÄ‚îÄ archive/                        # Historical snapshots
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ etl_dag.py                  # Airflow DAG definition
‚îÇ   ‚îú‚îÄ‚îÄ producers/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ producers.py            # Mock event emitters to Kafka topics
‚îÇ   ‚îî‚îÄ‚îÄ spark_jobs/
‚îÇ       ‚îî‚îÄ‚îÄ spark_consumer.py       # Spark structured streaming logic
‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ architecture.png            # Architecture diagram‚îî‚îÄ‚îÄ README.md

‚îÇ   ‚îî‚îÄ‚îÄ screenshots/
‚îÇ       ‚îú‚îÄ‚îÄ airflow_dag_success.png         #Airflow 
‚îÇ       ‚îú‚îÄ‚îÄ topics_created.png              #Terminal Output of topics.py
‚îÇ       ‚îî‚îÄ‚îÄ producers_running.png           #Terminal Output of producers.py


##  How to Run

> Prerequisites:
- Docker Desktop
- Python 3.11 (for Spark and Airflow)
- Git

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/data-platform.git
cd data-platform
````

### 2. Configure secrets

Create a `.env` file for your Gmail SMTP (for Airflow alerts):

```env
GMAIL_APP_PASSWORD=your_generated_app_password
```

### 3. Start Docker services

```bash
docker-compose up --build
```

Then visit Airflow UI at `http://localhost:8080`
Login with:
`Username: airflow`
`Password: airflow`

---

##  Testing

* Manual Run: Trigger the DAG manually from Airflow UI
* Daily Cron: DAG is scheduled to run every day at 2:00 PM GMT+3
* Output is written to `/output/` and archived to `/archive/`

---

## üõ†Ô∏è Troubleshooting & Production Tips

* Spark memory crashes? Restart your machine to reset JVM memory.
* Airflow DAG not showing up? Ensure DAG file is in `/dags/` folder that is volume-mounted inside container.
* Use `.env` + `.gitignore` to protect sensitive SMTP data.
* Don't run Airflow on Windows natively ‚Äî use Docker or WSL2.

---

##  Notes

* No cloud dependencies.
* Kafka topics and events are mock generated locally.
* Ideal for testing real-time event data pipelines offline.


##  Additional Docs

- [Full Project Documentation (PDF)](docs/project-documentation.pdf)

---

## License

MIT

